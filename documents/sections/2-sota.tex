% ================================================
% =              STATE OF THE ART                =
% ================================================ 

Currently, the automotive industry is driving the development of \aclink{ADS} with the promise of reducing road accidents and minimizing both the human and economic costs associated with them \cite{survey_AutomatedDriving1}. However, although automated driving has recently gained increased attention from the industry due to the rise of Deep Learning and computer vision, it has actually been present for more than 20 years.

Some of the earliest automated driving competitions, such as the DARPA Challenges in 2003 and 2005 or the Grand DARPA Urban Challenge in 2007, significantly boosted the development of \aclink{ADS}, attracting the attention of both technology companies and the automotive sector \cite{survey_AutomatedDriving2}. This progress has been accompanied by the establishment of best practices and standardization processes to ensure the safety and reliability of \aclink{ADS}. In this context, the \aclink{SAE} has defined a progressive scale of automation, ranging from Level 0 (no automation) to Level 5 (full automation), specifying the degree of driver intervention required at each stage \cite{AD_Technical_Standards}.

Today, most vehicles incorporate Advanced Driver Assistance Systems (\aclink{ADAS}), which operate at \aclink{SAE} Levels 2 and 3. However, Level 4 \aclink{ADS} already exist, such as those developed by Waymo and Cruise for robotaxis, as well as autonomous buses deployed in some cities. These systems are designed to manage fallback autonomously, without the need for human intervention \cite{fallback_strategy}.

This development has led to the creation of various strategies and architectures for \aclink{ADS}. In recent years, significant progress has been made in \mbox{End-to-End} solutions, which combine deep learning and reinforcement learning techniques to derive vehicle control actions directly from sensor data \cite{end_to_end_driving}. However, most approaches favor more traditional modular solutions, which divide the automated driving problem into specific \mbox{sub-tasks}, integrating solutions from fields such as robotics, computer vision, deep learning, and automatic control.

In the context of modular architectures, the adoption of best practices has facilitated the categorization of these \mbox{sub-tasks} into three main groups \cite{machines5010006}\cite{functional_architectures}:  

\begin{itemize}  
    \item \textbf{Perception}, which refers to the autonomous system's ability to gather environmental information and extract relevant knowledge, such as the location of obstacles, traffic signs, and the vehicle’s position.  
    \item \textbf{Behavior planning}, which involves making decisions to achieve the vehicle's objectives, such as reaching a destination while avoiding obstacles and optimizing the trajectory.  
    \item \textbf{Motion execution}, which pertains to the vehicle’s ability to carry out the planned actions by controlling steering, speed, and necessary maneuvers.  
\end{itemize}

Furthermore, these \mbox{sub-tasks} interact with each other, with the vehicle's hardware, and with communication systems such as \aclink{V2I} or \aclink{V2X} in the case of connected vehicles.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/sota/ADS_information_flow.png}
    \label{sota_ads_information_flow}
    \caption{Typical ADS modular architecture.}
\end{figure}

In these types of architectures, an error in one \mbox{sub-task} can propagate and affect the performance of others, potentially compromising the overall system functionality. This is particularly critical in the perception module, as the quality of the obtained information directly impacts subsequent tasks such as localization, mapping, and planning. Therefore, ensuring robust perception systems is essential for the performance and safety of \aclink{ADS}.

Two of the main perception tasks in \aclink{ADS} are 3D object detection and \aclink{BEV} segmentation. 3D object detection is one of the most crucial tasks and is commonly based on point clouds obtained from \aclink{LiDAR} sensors. In the absence of \aclink{LiDAR}, an alternative is camera-based 3D detection, which aims to predict 3D bounding boxes in a common coordinate system using images. \hl{Add references.}

On the other hand, \aclink{BEV} segmentation focuses on performing semantic segmentation of the environment, identifying drivable areas and lane boundaries in the vehicle's reference frame. Unlike object detection, \aclink{BEV} segmentation enables dense prediction of static environment classes, which is essential for local map construction, agent behavior estimation, and downstream tasks such as behavior planning in \aclink{ADS}. \hl{Add references.}

This thesis is developed within the context of \aclink{BEV} semantic segmentation \hl{...}.

\subsection{Semantic segmentation} \label{sota_semantic_segmentation}
There are multiple tasks in computer vision, each addressing different levels of scene understanding. Image classification assigns a single label to an entire image, while semantic segmentation works in the pixel-level of the image assigning a category to each pixel based on a predefined label set. Instance segmentation extends this by detecting multiple objects in an image and segmenting them to delineate their boundaries. The fusion of semantic and instnce segmentation is known as panoptic segmentation, which not only provides pixel-lebel semantic labels but also differentiates between instances of the same class. This thesis focuses on the semantic segmentation task, a well stablished research topic in computer vision.

Early methods relied on hand-crafted features but the introduction of Fully Convolutional Networks (\aclink{FCN}) \cite{FCNs} boosted semantic segmentation as convolutional layers in \aclink{FCN} enables a data-driven feature extraction from images enabling pixel-wise predictions suitable for segmentation tasks. Additionaly, U-Net \cite{u_net} further impoved this by incorporating an encoder-decoder architecture with skip connections to refine segmented object boundaries and preserve spatial information.

Furthermore, the introduction of image classification backbones like VGG \cite{VGG} and ResNet \cite{ResNet} into segmentation models further boosted performance. This backbones are known by their strong feature extraction capabilities, and also the residual connections in ResNets helped mitigate the vanishing gradient problem, improving the learning of deep features and enhancing segmentation results.

From that point, semantic segmentation started to been framed as a structured prediction problem and the development of specific modules and architectures for this task was made. That's the case of dilated convolution \cite{ditaled_conv}, which increase the receptive field by "inflating" the \aclink{CNN} kernels with holes allowing models to capture more spatial context in their prediction.

Also, Transformers have been adapted from NLP to vision tasks, starting with Vision Transformers (\aclink{ViT}) \cite{vit}, which model images as patch sequences using self-attention. For semantic segmentation, architectures like SETR \cite{SETR} replace \aclink{CNN} backbones with transformer for a better global understangin, while hybrid models like Swin Transformer \cite{swin} and MaskFormer balance efficiency and context modeling. These advancements improved segmentation performance, particularly in tasks requiring both fine details and long-range dependencies.

\hl{This needs a review. Maybe the ViTs needs a deeper explanation. SegFormer is introduced in methodology.}

\subsection{BEV semantic segmentation} \label{sota_BEV_semantic_segmentation}
Traditional methods \cite{3d_traffic_scene_understanding} estimate local \aclink{BEV} maps using camera inputs under the flat-ground assumption applying \aclink{IPM}. However, these methods require accurate camera parameters which has led to research focusing on camera parameter estimation for \aclink{BEV} transformation \cite{BEV_params_estimation1} \cite{BEV_params_estimation2}. Another key challenge in \aclink{BEV} map generation is handling object occupancy and occlusion. \aclink{ADS} must be aware of objects dimensions and should account for uncertainty in vehicular scenes. However, estimating vehicle occupancy is non-trivial as the necessary perspective views of objects are often unavailable. In this context, there are many reseachs that addresses the local semantic map estimation with different approaches.

Cam2BEV \cite{Cam2BEV} applies \aclink{IPM} to transform multi-camera segmented input images into the \aclink{BEV} domain which is feeded into the model to refine the \aclink{BEV} representation. To handle occlusions, Cam2BEV introduces an additional semantic class that explicitly marks occluded areas from all vehicle-mounted cameras. As the input of the model are already segmented images, an extra \aclink{CNN} was employed to test the method in real-world data.  

HDMapNet \cite{HDMapNet} generates high-definition semantic maps from multi-camera input by employing a feature projection module that transforms image features into \aclink{BEV} space. The model first extracts image features and transforms them into the camera coordinate system with a shared \aclink{MLP} backbone, and then projects them into \aclink{BEV} using camera extrinsics. Finally a semantic segmentation decoder is used.

PYVA \cite{PYVA} introduces a cross-view transformer that projects features from the front-view domain to the \aclink{BEV} domain. While similar to HDMapNet, PYVA differs in that it does not rely on camera parameters in the second transformation stage as the model is cappable of learning this transformation. Different to other methods, this work uses a GAN-based framework to manage occluions by estimating the vehicle's top view masks. However, this method is not suitable for multi-camera fusion.

Other approaches propose different architectures for \aclink{BEV} semantic segmentation. VPN \cite{view_parsing_network} introduces a two-layer \aclink{MLP} module for multi-camera feature fusion, followed by a decoder for semantic segmentation in indoor scenes. LSS \cite{lift_splat_shoot} proposes a unified framework that lifts 2D images into a 3D space by learning an implicit depth distribution and shows that their method is suitable for end-to-end motion plannig. M²BEV \cite{m2bev} transforms 2D image features into 3D voxels along projection rays and obtains an efficient \aclink{BEV} representation which supports multiple end tasks such as semantic segmentation or object 3D detection.

MonoLayout \cite{mono_layout} tackles occlusion estimation by employing a standard encoder-decoder framework combined with adversarial training, making it suittable for predicting amodal layouts of the driving scene. BEVFormer \cite{BEVFormer} similarly enhances occlusion reasoning by leveraging attention mechanisms to fuse multi-view spatial-temporal features from historical \aclink{BEV} maps.

There are also methods that combines information form multiple sensor such as FishingNet \cite{fishingnet} which extends VPN to use multiple sensors, or HDMapNet which is also cappable of using \aclink{LiDAR} sensors.  


In summary, existing approaches to local \aclink{BEV} map estimation typically follow one of two strategies: 
\begin{enumerate}
    \item Performing early-stage segmentation on input images before refining the \aclink{BEV} representation.
    \item Learning to embed image features into \aclink{BEV} space before passing them through a semantic segmentation decoder.
\end{enumerate}
However, to the best of our knowledge, no previous work directly trains a model on already-reprojected \aclink{BEV} images.

Instead of applying semantic segmentation before transforming images into \aclink{BEV} space, we study whether training a segmentation model directly on top-view images improves the representation of planar elements compared to the traditional segmentation-first-then-IPM approach. Furthermore, we treat occupancy and occlusion mask generation as a post-processing step applied to \aclink{BEV} semantic segmentation, where 3D object detection is performed. This is integrated into a pre-annotation pipeline for vehicle scenes, contributing to advancements in monocular 3D object detection. 

