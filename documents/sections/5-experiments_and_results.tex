% ================================================
% =                 EXPERIMENTS                  = 
% ================================================ 

This section includes all experiments carried out for evaluating the difference between the two approaches considered in the BEV2Seg\_2 pipeline, experiments to study what is the influence of extrinsic parameters modification as data augmentation technique for semantic segmentation of BEV images and the final evaluation of the proposed annotation pipeline of occupancy, occlusion and driveable areas.

\subsection{BEV2Seg\_2}

Both approaches, segmenting-then-IPM and IPM-then-segmenting, were trained under the same conditions. During fine-tuning for the segmentation task, encoder layers were left unfrozen, allowing the entire model to adapt to the training data. Input images were preprocessed using the \textit{SegformerImageProcessor}, which includes resizing to $512 \times 512$ pixels, rescaling pixel values by a factor of $1/255$, and normalizing with ImageNet mean and standard deviation values~\cite{imagenet}. This preprocessing step ensures the input format is consistent with what the pretrained encoders expect.

Semantic masks were provided during training with the \texttt{reduce\_labels} parameter set to \texttt{False}, as the dataset includes a "background" class. This configuration ensures that all pixel classes, including background, contribute to gradient computation during optimization. All experiments were executed on the hardware setup described in Table~\ref{tab:hardware}. From the six available Segformer encoder variants, three were selected for evaluation: MiT-b0, MiT-b2, and MiT-b4. Due to its larger size, the MiT-b4 model required two gradient accumulation steps to fit within the GPU's memory constraints. Checkpoints were saved based on evaluation loss, a design decision made despite ongoing discussions about whether loss or mean intersection over union is the better metric for model selection. A basic linear learning rate scheduler was applied throughout training and the loss function employed was \texttt{BCEWithLogitsLoss}, which is well-suited for multi-label semantic segmentation tasks, as it combines sigmoid activation and binary cross-entropy in a numerically stable way following the Segformer implementation.

\begin{table}[h]
    \centering
    \begin{tabular}{c l c}
        \toprule
        \textbf{Component} & \textbf{Specifications} & \textbf{Num workers} \\
        \midrule
        CPU         & Intel Xeon Gold 6230 (80) @ 3.900GHz & 8 \\
        GPU         & NVIDIA Tesla V100-SXM2-32GB & 2 \\      
        Memmory     & 772643MiB & - \\
        OS          & Ubuntu 22.04.3 LTS x86\_64 & - \\
        \bottomrule
    \end{tabular}
    \caption{ Hardware used for experiments }
    \label{tab:hardware}
\end{table}

Regarding to the used notation, the models that follow the segment-then-IPM pipeline to obtain BEV semantic masks are referred to as \texttt{raw2segbev}, while those that first apply IPM and then perform segmentation are named \texttt{raw2bevseg}.

The two approaches were firstly trained using the smallest Segformer model variant, MiT-b0, for $20.7K$ steps without applying any regularization technique. This initial experiment was performed to observe whether the models were able to learn and predict on the dataset and see if them suffered from overfitting. Also, for this purpose, the choice of MiT-b0 was intentional as it trains faster and, due to its limited capacity, is less prone to extreme overfitting compared to larger models. This made it a suitable candidate for testing different hyperparameter configurations in a lightweight environment.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{./images/experiments/overfitting_bev_nu.png}
    \caption{Training and evaluation loss of raw2segbev and raw2bevseg MiT-b0 models without any regularization technique}
    \label{fig:overfitting_mit-b0}
\end{figure}

As shown in Figure~\ref{fig:overfitting_mit-b0}, both models showed clear signs of overfitting. While the training loss continued to decrease continuously, the validation loss began to increase, indicating a lack of generalization and failure to converge. These results highlight the importance of introducing regularization techniques even for small model sizes. Additionally, no signs of exploding gradients were observed during the training of these models. 

Two main approaches were selected to tackle the overfitting problem: weight decay (also known as L2 regularization) and data augmentation. Weight decay penalizes large weights during training, and makes the model more robust and less prone to memorizing irrelevant details; while data augmentation techniques introduces variability in the training dataset enabling the model to adapt better to unseen data. However, the introduction of data augmentation techniques into \aclink{BEV} images domain is not trivial and raises another research question wich is tackled in Section~\ref{sec:data_augmentation}.

It is also important to highlight the difference in metrics between the two models. Even though the backbone and training hyperparameters remain the same, there is a noticeable difference in both the loss values and the \aclink{mIoU} between the models. This difference is primarily due to the fact that the evaluation datasets differ. Specifically, the approach that segments the image first and then reprojects it is evaluated using semantic masks in the camera view, while the approach that first reprojects images into \aclink{BEV} and then performs segmentation is evaluated using masks directly in the \aclink{BEV} domain.

This distinction motivates the analysis shown in Figure~\ref{fig:normal_vs_bev_evaluation}, where the model \texttt{raw2segbev\_mit-b0\_v0.3} is evaluated on the test set twice: once using semantic masks in the camera domain and once using \aclink{BEV} masks. The figure displays the per-class \aclink{mIoU} for all the semantic categories the model was trained on, and two main observations emerge.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{./images/experiments/raw2segbev_mit-b0_v0.2_new_test_evaluation.png}
    \caption{Model evaluated with normal and BEV images}
    \label{fig:normal_vs_bev_evaluation}
\end{figure}

First, there's a clear imbalance in performance between dominant classes such as \texttt{background} and \texttt{flat.driveable\_area}, and the remaining, less frequent classes. Many of the minority classes show extremely low or even zero \aclink{IoU} scores. This is partly due to the inherent class imbalance in the dataset: some semantic classes have no annotated pixels at all (see Appendix~\ref{appendix:OLDatasets} for more details).

Second, all metrics are significantly higher when the model is evaluated using standard camera-domain masks. Reprojecting the masks to BEV introduces several effects: (1) a fixed background area appears in the BEV space that lies outside the camera's field of view; (2) the maximum scene distance is reduced to $30$ meters (see reprojection parameters in Section~\ref{sec:bev2seg_2}), which limits the presence of distant objects: minority classes like pedestrians are often excluded unless they are very close to the vehicle, while driveable areas and vehicles dominate the scene; and (3) the resolution is reduced to $1024 \times 1024$ pixels.

All these factors lead to a lower total number of pixels in the BEV dataset. As a result, each misclassified pixel has a greater impact on the overall evaluation, making errors more costly and consequently lowering the average performance metrics.

To address this, many training methods use class weighting in the loss function to penalize misclassifications of minority classes more heavily than those of majority classes. However, in this particular project, it is not essential to maintain all class distinctions provided by the dataset. Therefore, we investigate whether merging semantic classes can help mitigate the effects of class imbalance.

With all of this in place, the training strategy is designed to support experimentation in order to answer three main research questions:

\begin{itemize}
    \item \textit{Does the label's merging strategy helps increasing the model's performance on the low presence classes?}
    \item \textit{Which data augmentation technique is more effective for trainig a model directly on \aclink{BEV} images on a semantic segmentation task?}
    \item \textit{Which of the two approaches performs better for BEV driveable area segmentation?}
\end{itemize}

\hl{Mention the used hyperparameters for training the models.}

\subsubsection{Merging labels} \label{sec:merging_labels}
As previously discussed, to improve model performance on the dataset’s minority classes, a semantic label merging strategy was applied. This approach combines less represented semantic classes into more general, representative categories. Notably, this same merging strategy was already used in the annotation pipeline, so it was incorporated during training using the same look-up table of merging rules described in Table 3. The training and evaluation scripts were adapted accordingly to apply the merging rules consistently.

Table~\ref{tab:merging_comparison_bev} shows the results of two models trained with camera-perspective images and evaluated on the same test set using \aclink{BEV} masks. Model A was trained using the full set of original semantic classes, while Model B was trained using the merged classes, thereby reducing the total number of labels. 

In general, the results indicate that both models achieve comparable performance on the majority classes, specifically \texttt{background} and \texttt{flat.driveable\_surface}. However, a significant difference is observed in the minority classes, where Model B outperforms Model A across all evaluation metrics. This suggests that simplifying the semantic space does not decrease the model's ability to identify important instances and, instead, it reduces the penalty from underrepresented classes. Additionally, by reducing the number of classes, the overall average of the metrics improves as it avoids dilution from irrelevant or sparsely populated classes. This makes label merging especially useful in cases where fine-grained segmentation isn't necessary and consistency across the whole model is more important.

\begin{table}[h]
    \centering
    \tiny
    \begin{tabular}{llcccc}
    \toprule
    \textbf{Merged Class} & \textbf{Original Label} & \textbf{mIoU A} & \textbf{mIoU B} & \textbf{mF1 A} & \textbf{mF1 B} \\
    \midrule
    background & background & 0.97 & 0.97 & 0.99 & 0.99 \\
    \midrule
    animal & animal & 0.00 & 0.00 & 0.00 & 0.00 \\
    \midrule
    human.pedestrian.adult & human.pedestrian.adult & 0.07 & 0.08 & 0.09 & 0.11 \\
     & human.pedestrian.child & 0.00 & - & 0.00 & - \\
     & human.pedestrian.construction\_worker & 0.00 & - & 0.00 & - \\
     & human.pedestrian.personal\_mobility & 0.00 & - & 0.00 & - \\
     & human.pedestrian.police\_officer & 0.00 & - & 0.00 & - \\
     & human.pedestrian.stroller & 0.00 & - & 0.00 & - \\
     & human.pedestrian.wheelchair & 0.00 & - & 0.00 & - \\
    \midrule
    movable\_object.barrier & movable\_object.barrier & 0.08 & 0.14 & 0.09 & 0.17 \\
     & movable\_object.debris & 0.00 & - & 0.00 & - \\
     & movable\_object.pushable\_pullable & 0.00 & - & 0.00 & - \\
     & movable\_object.trafficcone & 0.08 & - & 0.10 & - \\
     & static\_object.bicycle\_rack & 0.00 & - & 0.00 & - \\
    \midrule
    vehicle.car & vehicle.bus.bendy & 0.00 & - & 0.00 & - \\
     & vehicle.bus.rigid & 0.02 & - & 0.03 & - \\
     & vehicle.car & 0.30 & 0.38 & 0.33 & 0.42 \\
     & vehicle.construction & 0.01 & - & 0.01 & - \\
     & vehicle.emergency.ambulance & 0.00 & - & 0.00 & - \\
     & vehicle.emergency.police & 0.00 & - & 0.00 & - \\
     & vehicle.trailer & 0.00 & - & 0.00 & - \\
     & vehicle.truck & 0.08 & - & 0.09 & - \\
    \midrule
    vehicle.ego & vehicle.ego & 0.00 & 0.00 & 0.00 & 0.00 \\
    \midrule
    vehicle.motorcycle & vehicle.bicycle & 0.02 & - & 0.02 & - \\
     & vehicle.motorcycle & 0.04 & 0.06 & 0.05 & 0.07 \\
    \midrule
    flat.driveable\_surface & flat.driveable\_surface & 0.94 & 0.94 & 0.97 & 0.97 \\
    \bottomrule
     & & 0.10 & 0.32 & 0.11 & 0.34 \\
    \bottomrule
    \end{tabular}
    \caption{Per-class metric comparison between Model A and merged Model B evaluated with BEV images}
    \label{tab:merging_comparison_bev}
\end{table}
    



\subsubsection{Data augmentation} \label{sec:data_augmentation}
Data augmentations are commonly used in deep learning models to mitigate overfitting during training and improve model generalization. There exists multiple types of data augmentation on the image domain: from pixel-based transformations, such as color space modifications, histogram equalization or filtering operations; to geometric transformations, including translations, rotations, shearings and homographies. These techniques have been widely applied in computer vision tasks and have shown to enhance model performance. However, performing data augmentation in \aclink{BEV} is not an easy task, as \aclink{IPM} images are already homographies of camera images, resulting in inherent distorsions.

Filtering operations can be applied to both standard and \aclink{BEV} images but geometric transformations were selected as the primary data augmentation method for camera domain images following the strategies employed in training the SegFormer model \cite{segformer}. Accordingly, random resizing, random cropping, and horizontal flipping were chosen as augmentation operations for perspective images.

Regarding \aclink{BEV} data augmentations, some multi-view methods implement strategies such as random flipping and random scaling, while others operate in the frequency domain \cite{HSDA}. However, these approaches apply augmentations to perspective images before the BEV transformation. Performing random cropping on a \aclink{BEV} image may lead to significant information loss, as large portions of the image may consist of unlabeled background data, potentially resulting in crops with insufficient information for effective training (Figure~\ref{fig:bev_cropping}). 

\begin{figure}[h]
    \centering
    % Row labels
    \setlength{\tabcolsep}{1pt}  % Reduce column padding
    \renewcommand{\arraystretch}{0.5}
    \begin{tabular}{c c c c c c c c}

        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_0_bev.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_1_bev.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_2_bev.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_3_bev.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_4_bev.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_5_bev.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_6_bev.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/mini/mini_7_bev.png} \\ 
        
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_0_9.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_1_2.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_2_5.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_3_5.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_4_3.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_5_2.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_6_1.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_image_7_2.png} \\ 

        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_0_9.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_1_2.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_2_5.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_3_5.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_4_3.png} & 
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_5_2.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_6_1.png} &
        \includegraphics[width=0.12\textwidth]{images/methodology/data_augmentations/bev_crop_mask_7_2.png} \\

    \end{tabular}
    
    \caption{Random cropping and horizontal flipping on BEV images. Original BEV images on first row; random flipped and cropped images on second row and corresponding semantic masks on last row.}
    \label{fig:bev_cropping}
\end{figure}

In this context, a different approach was also considered: applying geometric transformations by modifying the camera's extrinsic parameters before reprojecting to \aclink{BEV} space. The objective is to introduce random transformations along one of the camera’s rotation axes, generating diverse \aclink{BEV} reprojections with varying degrees of distortion. This technique may enable the model to adapt to different extrinsic camera configurations, improving its robustness to variations in camera placement and orientation (Figure~\ref{fig:bev_data_aug}).

\begin{figure}[h]
    \centering
    % Row labels
    \setlength{\tabcolsep}{1pt}  % Reduce column padding
    \renewcommand{\arraystretch}{0.5}
    \begin{tabular}{c c c c c c}
        & $-0.25$ rad & $-0.125$ rad & $0.0$ rad & $0.125$ rad & $0.25$ rad \\ 
        
        \rotatebox{90}{\textbf{Yaw}} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rx_-0.25_0.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rx_-0.125_1.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rx_0.0_2.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rx_0.125_3.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rx_0.25_4.png} \\ 
        
        \rotatebox{90}{\textbf{Pitch}} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rz_-0.25_0.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rz_-0.125_1.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rz_0.0_2.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rz_0.125_3.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/rz_0.25_4.png} \\ 

        \rotatebox{90}{\textbf{Roll}} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/ry_-0.25_0.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/ry_-0.125_1.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/ry_0.0_2.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/ry_0.125_3.png} & 
        \includegraphics[width=0.15\textwidth]{images/methodology/data_augmentations/ry_0.25_4.png} \\ 
        
    \end{tabular}
    
    \caption{Effect of camera transformations on BEV projection. The first row shows variations in the yaw axes, the second in pitch, and the third in roll.}
    \label{fig:bev_data_aug}
\end{figure}


This section has a dual purpose. First, the effect of applying geometric transformations to camera-perspective images as data augmetation strategy for training a traditional segmentation model in order to mitigate overfitting is studied. Second, the effectiveness of two described techniques for performing data augmentation on \aclink{BEV} images is explored: applying the same geometric augmentations used for regular images, or modifying the camera's extrinsic parameters before performing the projections.

Therefore, the introduction of data augmentation strategies such as random cropping, horizontal flipping, and rescaling on regular images significantly reduces overfitting, as shown in Figure~\ref{fig:normal_before_after_data_aug_logs}. This reduction in overfitting allows for longer training periods and improves model performance on the evaluation set, leading to convergence at higher \aclink{mIoU} values. This effect is also reflected in the evaluation of the models on the test set, where the model with data augmentation presents higher \aclink{mIoU} than the model affected by overfitting.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{./images/experiments/nor_before_after_data_aug_logs.png}
    \caption{Before and after regularization techniques on models trained with camera-domain images}
    \label{fig:normal_before_after_data_aug_logs}
\end{figure}

Secondly, the Figure~\ref{fig:bev_before_after_data_aug_logs} shows the trainig process graph for the three models trained with \aclink{BEV} images and reveals some significant differences. The first model, without any data augmentation, exhibits notable overfitting. In contrast, both data augmentation strategies, the traditional geometric augmentations (random flipping, random cropping, and rescaling) and the custom strategy of modifying the camera's extrinsic parameters, seems to partially reduce this overfitting.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{./images/experiments/bev_before_after_data_aug_logs.png}
    \caption{Training logs of models with BEV data augmentation techniques}
    \label{fig:bev_before_after_data_aug_logs}
\end{figure}

Focusing on the model employing traditional data augmentation techniques on the BEV images, the training error is generally higher, and the training and validation error curves remain closely aligned with a considerably lower mIoU on the validation set compared to the other two models. This pattern suggests potential underfitting. This observation appears to support the initial hypothesis that directly applying traditional image augmentations to BEV representations might not provide the model with sufficiently informative variations for effective training in this specific domain.

On the other hand, the model trained with the extrinsic's data augmentation strategy shows less overfitting than the model without any data augmentation technique. This allows for more extended training and achieves superior results on the validation set, outperforming the overfitted model. 

Overall, the findings from the validation set suggest that the custom data augmentation techniques for BEV images offer better performance than directly applying traditional augmentations. However, it is crucial to examine the models' performance on the unseen test set. Figure~\ref{fig:bev_before_after_data_aug_test} illustrates this evaluation, revealing that the performance difference between the three models is not as substantial in reality. While the model trained with extrinsic's parameters modification shows slightly better results, the performance gap between the other two models is less clear, with the model using traditional data augmentation on BEV images occasionally surpassing the overfitted model in vehicle-related classes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{./images/experiments/bev_before_after_data_aug_test.png}
    \caption{Different data augmentation stategies evaluated with BEV images.}
    \label{fig:bev_before_after_data_aug_test}
\end{figure}

\subsubsection{raw2segbev vs raw2bevseg}
The Table~\ref{tab:model_comparison} shows a comparative analysis of semantic segmentation performance across different models, categorized into two distinct \aclink{BEV} mask generation pipelines: "\aclink{IPM} Then Segmenting" and "Segmenting Then \aclink{IPM}". Values represent the \aclink{mIoU} for the merged semantic classes (Section~\ref{sec:merging_labels}), with "Mean" indicating the average \aclink{mIoU} across all displayed classes for each model. Also, models are categorized by their architecture (MiT-B0, MiT-B2, MiT-B4) and the used data augmentation technique (O: No data augmentation, overfitted model; N: Normal data augmentation; E: Camera's extrinsic parameters modification)

\begin{table}[h]
    \centering

    \begin{threeparttable}
        \tiny
        \newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
        \begin{tabular}{l | C{0.7cm} C{1.3cm} C{1cm} C{1.2cm} C{1cm} C{0.7cm} C{0.7cm} C{1.3cm} | c}
            \toprule
            \textbf{Model} & \textbf{animal} & \textbf{background} & \textbf{\makecell{flat.\\driveable\\\textunderscore surface}} & \textbf{\makecell{human.\\pedestrian.\\adult}} & \textbf{\makecell{movable\\\textunderscore object.\\barrier}} & \textbf{\makecell{vehicle.\\car}} & \textbf{\makecell{vehicle.\\ego}} & \textbf{\makecell{vehicle.\\motorcycle}} & \textbf{Mean} \\
            \midrule

            \multicolumn{10}{l}{\textbf{IPM Then Segmenting}} \\
            \midrule
            \multirow{1}{*}{\text{MiT-B0 O}}  & 0.00 & 0.97 & 0.93 & 0.04 & 0.14 & 0.32 & 0.00 & 0.04 & 0.30 \\
            \multirow{1}{*}{\text{MiT-B0 E}}  & 0.00 & 0.97 & 0.94 & 0.04 & 0.14 & 0.34 & 0.00 & 0.04 & 0.31 \\
            \multirow{1}{*}{\text{MiT-B2 E}}  & 0.00 & 0.97 & 0.95 & 0.05 & 0.17 & 0.36 & 0.00 & 0.05 & 0.32 \\
            \multirow{1}{*}{\text{MiT-B4 E}}  & 0.00 & 0.98 & 0.95 & 0.06 & 0.17 & 0.36 & 0.00 & 0.06 & 0.32 \\
            \multirow{1}{*}{\text{MiT-B0 N}}  & 0.00 & 0.97 & 0.94 & 0.03 & 0.14 & 0.32 & 0.00 & 0.03 & 0.30 \\
            \multirow{1}{*}{\text{MiT-B2 N}}  & 0.00 & 0.97 & 0.94 & 0.04 & 0.15 & 0.33 & 0.00 & 0.04 & 0.31 \\
            \multirow{1}{*}{\text{MiT-B4 N}}  & 0.00 & 0.97 & 0.94 & 0.05 & 0.14 & 0.33 & 0.00 & 0.04 & 0.31 \\
            \midrule[1pt]

            \multicolumn{10}{l}{\textbf{Segmenting Then IPM}} \\
            \midrule
            \multirow{1}{*}{\text{MiT-B0 O}}  & 0.00 & 0.97 & 0.94 & 0.08 & 0.14 & 0.38 & 0.00 & 0.06 & 0.32 \\
            \multirow{1}{*}{\text{MiT-B0 N}}  & 0.00 & 0.98 & 0.95 & 0.09 & 0.18 & 0.45 & 0.00 & 0.06 & 0.34 \\
            \multirow{1}{*}{\text{MiT-B2 N}}  & 0.00 & 0.98 & \textbf{0.95} & \textbf{0.11} & \textbf{0.20} & \textbf{0.48} & 0.00 & 0.08 & \textbf{0.35} \\
            \multirow{1}{*}{\text{MiT-B4 N}}  & 0.00 & \textbf{0.98} & 0.95 & 0.11 & 0.20 & 0.47 & 0.00 & \textbf{0.08} & 0.35 \\
            \bottomrule
        \end{tabular}

    
        % Notas de tabla
        \begin{tablenotes} 
            \item[] O: Overfitted. N: Normal. E: Extrinsic.
        \end{tablenotes}
    \end{threeparttable} 


    \caption{mIoU models comparison.}
    \label{tab:model_comparison}
\end{table}


A direct comparison of the two pipelines reveals a clear advantage for segmenting in the original perspective before reprojecting to \aclink{BEV}. This approach consistently yields higher overall \aclink{mIoU} values, ranging from $0.32$ to $0.35$. In contrast, the "IPM Then Segmenting" method, which transforms images to \aclink{BEV} prior to segmentation, consistently results in lower \aclink{mIoU} scores. This strongly suggests that performing semantic segmentation in the native image space is generally more effective for preserving semantic information.

Examining class-wise performance highlights key strengths and weaknesses across both methodologies. Both strategies consistently excel in segmenting drivable surfaces, achieving high \aclink{mIoU} scores, often above $0.90$. Also, a consistent \aclink{mIoU} of $0.00$ is observed for \texttt{animal} and \texttt{vehicle.ego} across all models and pipelines. This specific zero performance is not indicative of a model or pipeline failure, but rather, derive from insufficient pixel representation for these categories within the dataset itself.

Beyond these high-performing or data-limited classes, the "Segmenting Then IPM" pipeline demonstrates significant improvements in categories that posed greater challenges. \texttt{human.pedestrian.adult} notably sees its \aclink{mIoU} more than double to $0.11$ (MiT-B2 N and MiT-B4 N). Similarly, \texttt{movable\_object.barrier} and \texttt{vehicle.car} experience marked increases, with \texttt{vehicle.car} achieving a peak of $0.48$ by MiT-B2 N (the highest score for any model in this class across all tested configurations). While still a very low score, \texttt{vehicle.motorcycle} also shows modest improvement, reaching $0.08$ with MiT-B4 N in this more effective pipeline. These improvements suggest that preserving detailed object features and context from the original image view is particularly beneficial for more complex and often smaller or less consistently represented objects in \aclink{BEV}.

Regarding model architecture, a subtle trend of increasing performance is observed with larger MiT models (from B0 to B4) within each pipeline, though these gains are generally marginal compared to the impact of the pipeline choice itself. Within the "Segmenting Then IPM" group, the MiT-B2 N model stands out as the best overall performer, achieving the highest mean mIoU of $0.35$ and leading in several critical classes including \texttt{flat.driveable\_surface}, \texttt{human.pedestrian.adult}, \texttt{movable\_object.barrier}, and \texttt{vehicle.car}. 

This comprehensive analysis underscores the critical influence of the BEV transformation strategy on semantic segmentation accuracy, highlighting that segmenting in the native image space prior to reprojection is a more effective approach for achieving robust results across a diverse range of semantic classes.


\subsection{Annotation evaluation}
For the pipeline's evaluation, NuScenes scene 'scene-0061' \footnote{\url{https://youtu.be/4gkyUWSZUkg?si=y5D9iybgzkv37Bko}} was used. This scene, captured in 'singapore-onenorth', includes object annotations, map data, and odometry at 2Hz for a total of 39 frames. The scene features a parked truck, a construction area, an intersection requiring a left turn, and the ego-vehicle following a van. The ego-vehicle's path is shown in Figure~\ref{fig:scene_ego_path}.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{./images/experiments/ground_truth_turning_5deg.png}
    \caption{Scene ego-vehicle's route}
    \label{fig:scene_ego_path}
\end{figure}

The ego-vehicle starts next to a truck, visible by the front camera for the first 4 frames (with only a small corner of the truck visible in the fourth). Furthermore, in the second frame of the scene, a vehicle is already visible in the right lane, and from the third frame onwards, the van that takes the left exit and that the ego-vehicle will follow until the end of the scene begins to be visible. Throughout the entire segment, several occlusions and partial visualizations of the objects occur, making it a sufficiently representative situation to evaluate the pipeline's output and identify its strengths and weaknesses.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{\textbf{Predicted}} \\
    \cline{2-3}
     & Positive & Negative \\
    \hline
    \textbf{Actual Positive} & 92 (TP) & 15 (FN) \\
    \hline
    \textbf{Actual Negative} & 23 (FP) & 0 (TN) \\
    \hline
    \end{tabular}
    \caption{True positives, False positives and False negatives}
    \label{tab:scene_confusion_matrix}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        % \textbf{Category} & \textbf{BBD} & \textbf{DED-x (m)} & \textbf{DED-y (m)} & \textbf{DED-z (m)} \\
        \textbf{Category} & \textbf{DED-x (m)} & \textbf{DED-y (m)} & \textbf{DED-z (m)} \\
        \hline
        
        \multicolumn{4}{|l|}{\textbf{Overall Mean}} \\
        \hline
        % All Objects & 1.11 & 3.06 & 0.99 & 0.83 \\
        All Objects & 3.06 & 0.99 & 0.83 \\
        \hline

        \multicolumn{4}{|l|}{\textbf{Metrics Grouped by Alignment}} \\
        \hline
        % Aligned & 0.91 & 2.34 & 0.43 & 0.27 \\
        % Not aligned & 1.19 & 3.32 & 1.20 & 1.03 \\
        Aligned & 2.34 & 0.43 & 0.27 \\
        Not aligned & 3.32 & 1.20 & 1.03 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        % \item[] BBD: Bounding Box Disparity.
        \item[] DED: Difference in Each Dimension.
    \end{tablenotes}
    \caption{Mean Dimensional Error Differences (DED) for 3D Object Detections}
    \label{tab:combined_ded_metrics}
\end{table}


\subsubsection{Overall performance}
Each object in the scene, encompassing both ground truth annotations and the pipeline's detections, is assigned a unique identifier. As detailed in Section X (replace X with your section number/label), the evaluation process associates detections with annotated objects. This results in a variable number of metrics per frame, with each metric derived from a ground-truth annotation and pipeline detection pair. This evaluation was conducted with a Field of View (FOV) range of 30 meters. The volume-to-volume (v2v) distance was employed as the association metric, with the maximum association distance established at 3.0 meters.

Using these parameters, the confusion matrix presented in Table [tab:confusion\_matrix] (replace tab:confusion\_matrix with your table label) was generated. This table aggregates all instances between detections and the visible ground truth across all frames of the scene. In this context, a True Positive (TP) signifies a detection correctly associated with a ground truth object. A False Positive (FP) indicates a detection that was not associated with any ground truth object, and a False Negative (FN) represents a ground truth object that the pipeline failed to detect.
The accumulated confusion matrix reveals that out of 107 ground truth objects within the camera's FOV, the system successfully estimated 92. This leaves 15 ground truth objects either undetected or with highly noisy predictions. Notably, the system also generated 23 FPs, corresponding to noisy predictions (often due to inadequately filtered object point clouds) for objects not present in the ground truth.
Overall, the Precision of the pipeline is approximately 80.0\%. This indicates that when the system predicts an object, it is correct 80\% of the time, suggesting a relatively low rate of false positives. The Recall is approximately 86.0\%. This means the pipeline successfully detected 86\% of all actual 3D objects present in the scene, demonstrating its ability to find most of the relevant objects.

\subsubsection{Dimension estimation accuracy}
Among all TPs, an evaluation of dimension estimation accuracy was performed, with the results compiled in Table [tab:dimension\_accuracy\_metrics] (replace tab:dimension\_accuracy\_metrics with your table label). This table also presents metrics for objects and frames, grouped according to their alignment with the vehicle's front camera. This grouping is determined by the yaw angle difference between the annotated ground truth objects and the vehicle's orientation. An object is considered not aligned if this angular difference exceeds 15 degrees.

Observing the mean total difference in dimensions for each axis (X, Y, Z of the vehicle's coordinate system), the X-axis (corresponding to the camera's depth axis) exhibits a markedly higher error rate compared to the Y-axis (width) and Z-axis (height). This suggests that the object depth estimation is considerably less accurate than height and width estimations. This observation is further substantiated by the metrics grouped by camera alignment. Objects aligned with the camera show a significantly higher error in depth estimation, whereas height and width estimations are relatively close to the ground truth (mean errors of 43cm and 27cm for width and height estimation, respectively). Conversely, objects not aligned with the camera display higher error rates not only in depth but across all axes. This reflects an over-estimation of the footprint (width and depth) when objects are not aligned with the camera, as illustrated in the specific case shown in Figure [fig:footprint\_overestimation\_example] (replace fig:footprint\_overestimation\_example with your figure label).
However, for non-aligned objects, one might anticipate the height error to remain relatively stable, as the height should be largely unaffected by a Z-axis rotation. This discrepancy prompted a more detailed analysis focusing on the metrics for each ground truth object for which detections were made.

To illustrate different facets of the annotation system's performance in object dimension estimation, three specific cases are highlighted: the parked truck at the scene's commencement (ID 41); the van preceding the ego-vehicle (ID 154); and an excavator stationary mid-curve (ID 129). These three objects are depicted in Figure [fig:case\_study\_object\_representation] (replace fig:case\_study\_object\_representation with your figure label). The evolution of dimensional estimation errors for these objects on a per-frame basis is presented in Figure [fig:dimension\_error\_evolution] (replace fig:dimension\_error\_evolution with your figure label), which also indicates via a horizontal bar whether the object is aligned with the camera in the current frame.

For the parked truck, the length estimation error is initially around 2 meters. However, in the final frames of its visibility, this error escalates to nearly 7 meters. This is mainly because the truck is parked and the ego-vehicle overtakes it, causing the visible area of the truck to change considerably in each frame. Frame 2 provides the most visible information, while frame 5 provides the least, as only the side of the truck's front is visible, leading to a highly inaccurate instantaneous depth estimation for the vehicle.

The evolution of dimensional errors for the van exemplifies the system's typical behavior concerning object alignment with the camera. In the initial frames, when the van is aligned, there is a high error in length estimation but lower errors in height and width. As the van progressively rotates relative to the camera, the length estimation error diminishes, accompanied by an increase in the width estimation error. The height estimation error remains relatively constant, barring some outliers. When the van realigns with the camera, the predominant error reverts to length estimation. This behavior underscores a significant limitation of the annotation system when employing Axis-Aligned Bounding Boxes (AABBs).

The case of the parked excavator is notable due to an unusually high height estimation error, consistently exceeding 2 meters in almost all frames where it is visible. This is closely linked to occlusion: as shown in Figure [fig:excavator\_occlusion\_detail] (replace fig:excavator\_occlusion\_detail and specify frame X in your caption), which visualizes the ground truth and detection in frame X, the van interposes itself between the ego-vehicle and the excavator. Consequently, only the upper part of the excavator is visible, significantly impairing its height estimation.

\subsubsection{Position estimation accuracy}
The Bounding Box Disparity (BBD) serves as a valuable metric for assessing the positional accuracy of cuboids and for readily identifying potential incorrect detections. For instance, Figure [fig:van\_bbd\_evolution\_graph] (replace fig:van\_bbd\_evolution\_graph with your figure label) illustrates the BBD evolution for the van throughout the scene. The horizontal line in this graph denotes the BBD metric's intrinsic threshold separating Intersection over Union (IoU) based calculations from Volume-to-Volume (V2V) based ones. An example is the initial detection of the van in frame 2, which exhibits a BBD greater than 1. This signifies that the detection does not intersect with the ground truth bounding box, as depicted in Figure [fig:van\_bbd\_no\_intersection\_example] (replace fig:van\_bbd\_no\_intersection\_example with your figure label). Generally, a high BBD can indicate an erroneous detection, as observed in frames 12 and 13 for this object.


\hl{Complete this bullet points.}
\begin{itemize}
    \item Explicar escena seleccionada
    \item Explicar parámetros de evaluación 3D (fov distance...)
    \item Explicar asignaciones (un pequeño análisis de los true positives, false positives y false negatives)
    \item Explicar el caso de la diferencia absoluta en dimensiones (agrupando por alineación con la cámara)
    \item Explicar el caso del objeto 129 que tiene una oclusion parcial siendo únicamente visible la parte superior del vehículo haciendo que el error en el eje vertical sea muy superior a las otras dimensiones
    \item Explicar el caso 154 en el que se refleja muy bien como cuando el vehículo está alineado con la cámara el eje que peor se estima es el de profundidad, mientras que cuando el objeto rota estos ejes cambian. Esto demuestra de forma clara la limitación del sistema de utilizar AABB.
    \item Introducir BBD para identificar outliers y demás (hay que hacer un análisis sencillo)
\end{itemize}




