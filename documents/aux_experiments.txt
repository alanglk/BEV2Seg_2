¿Se ha explicado qué es una máscara semántica?

[3.1.3 Validation and comparison]

A la hora de validar y comparar distintos modelos de segmentación es fundamental utilizar una base de datos común y emplear métricas adecuadas que permitan evaluar el rendimiento de forma cuantitativa y objetiva. El primer requisito se garantiza mediante el uso del conjunto de test del dataset, el cual no ha sido previamente expuesto a los modelos a comparar, asegurando así una evaluación imparcial y representativa de su capacidad de generalización.

En cuanto a las métricas de evaluación, si observamos la Figura X, donde se muestra una máscara semántica inferida por un modelo de segmentación junto con la correspondiente máscara anotada (ground truth), podemos intuir que una predicción acertada será aquella que maximice la superposición entre la máscara predicha y la máscara real, evitando al mismo tiempo una sobreextensión excesiva más allá de los límites de las regiones de interés del ground truth.

Con este objetivo se emplean dos métricas distintas pero estrechamente relacionadas: los índices de Dice (también conocido como F1-score) y de Jaccard (también conocido como Intersection over Union, IoU), cuyas expresiones formales se presentan en la Fórmula X. Ambas métricas están acotadas en el rango [0, 1], donde un valor de 0 indica ausencia total de solapamiento entre predicción y ground truth, y un valor de 1 indica coincidencia perfecta entre ambas máscaras.

Supongamos que partimos de una imagen de resolución H×W y queremos predecir N clases distintas. La salida del modelo será un tensor de dimensiones HxWxN, es decir, N máscaras de tamaño HxW en las que cada elemento representa un valor de pertenencia a una clase determinada. Sin embargo, en la segmentación semántica se considera únicamente la mejor predicción para cada píxel, generando así N máscaras binarias que representan las clasificaciones semánticas finales de la imagen original. En este contexto, las métricas descritas se calculan de manera independiente para cada una de las N clases (hard metrics).

Para un ground truth fijo, ambas métricas están positivamente correlacionadas: si un clasificador A supera a un clasificador B según una de las métricas, también lo superará según la otra. Este hecho podría llevar a pensar que la elección entre una u otra métrica es arbitraria; sin embargo, las diferencias se hacen evidentes al calcular la media de las métricas sobre un conjunto de inferencias. En este punto, aunque ambas métricas coincidan en que el modelo A tiene mejor rendimiento que el modelo B, difieren en la magnitud de dicha diferencia.

En términos generales, la métrica IoU tiende a penalizar con mayor severidad las instancias mal clasificadas en comparación con el F1-score (o Dice score), proporcionando una medida más cercana al rendimiento en el peor de los casos. Por su parte, el F1-score tiende a reflejar un rendimiento más cercano al promedio. Esta diferencia implica que, al calcular promedios sobre múltiples inferencias, el IoU será más sensible a errores puntuales significativos, mientras que el F1-score será más tolerante ante estos casos aislados.

Con todo esto se ha elegido mean intersection over union como la métrica principal para evaluar y comparar modelos en este trabajo.



[4.1 Después de las medidas propuestas para solucionar el overfitting]

Además, es importante destacar la diferencia de métricas entre los dos modelos. A pesar de que el backbone y los hiperparámetros de entrenamiento se hayan mantenido iguales, es notoria la diferencia en los losses y mean intersection over union que presentan ambos modelos. Esto se debe principalmente a que los datasets de evaluación son distintos ya que el approach de segmentar primero una imagen para luego reproyectarla se evalúa con máscaras semánticas en el dominio de la cámara, mientras que el approach de reproyectar imágenees y luego segmentar se evalua directamente con máscaras en BEV.

Este hecho motiva la gráfica de la Figura 16, en la que se ha tomado el modelo 'raw2segbev_mit-b0_v0.3' y se ha evaluado con el conjunto de test primero con máscaras semánticas en el dominio de la cámara y luego con máscaras BEV. En esta Figura se puede apreciar en mayor detalle el valor de mean intersection over union por cada una de las clases semanticas para las que se ha entrenado el modelo y, en general, destacan dos hechos.

En primer lugar, existe un desbalanceo notable entre las métricas de las clases mayoritarias como 'backgroun' o 'flat.driveable_area' y el resto de clases minoritarias y, además, muchas otras clases tienen valores muy cercanos a 0 o nulos. Esto refleja en parte el desbalanceo inherente al dataset, siendo que hay clases semánticas que no tienen ningún pixel anotado en el mismo (ver Anexo A).

En segundo lugar, todos los valores de métricas son notablemente superiores en el modelo siendo evaluado con máscaras normales. Al realizar la reproyección de las máscaras a BEV ocurren tres hechos: se añade una zona de 'background'fija siendo el área BEV que está fuera del FOV de la cámara; se acorta la distancia máxima de la escena representada en la imagen BEV haciendo que únicamente los objetos presentes a una distancia menor a $15m$ (ver parámetros de reproyección en la Sección 3) estén presentes en la imagen, lo que acentúa que las clases minoritarias no se encuentren (aparece sobre todo el área conducible y los coches pero los peatones a no ser que se encuentren muy cerca y delante del vehículo no aparecen); y la resolución de la imagen disminuye a $1024x1024$. 

Todos estos factores hacen que la cantidad total de píxeles sea menor en el conjunto BEV provocando que los errores al cometer una mala predicción en un pixel sea notablemente más costoso que al evaluar con el dataset de máscaras normales (haciendo a su vez que las métricas medias sean mucho menores). 

Para solucionar esto, muchos métodos de entrenamiento emplean pesos asociados a cada clase para que a la hora de computar el loss en cada iteración del entrenamiento el efecto de cometer un error en la predicción de una clase minoritaria sea sustancialmente superior al predecir mal una clase mayoritaria. Sin embargo, debido a la naturaleza del proyecto, no es necesario tener una segmentación en clases tan precisas como las que proporciona el dataset, por lo que se ha decidido explorar si al realizar un merge de las clases semánticas se minimiza el efecto del desbalanceo de las clases.


[4.1.1 Merging labels]
Como se ha comentado, con el fin de mejorar el rendimiento de los modelos en las clases minoritarias del dataset se ha decidido hacer un merge de las clases semánticas en clases más representativas. Esta estrategia es la mima en la empleada en el pipeline de anotación implementado, por lo que se ha decidido implementarlo en tiempo de entrenamiento utilizando el mismo conjunto look up table de reglas de merging semánticos de labels que el descrito el la tabla 3 adaptando los scripts de entrenamiento y evaluación para considerar el merging look up table.

En la Figura X se presentan los resultados comparativos de dos modelos evaluados sobre el mismo conjunto de test con máscaras en vista BEV. El Modelo A ha sido entrenado con todas las clases semánticas originales, mientras que el Modelo B utiliza la estrategia de merging, reduciendo así el número total de clases consideradas. Los resultados muestran que, en algunas clases específicas, el Modelo A logra un mayor valor de Intersection over Union (IoU); sin embargo, en esos mismos casos, el F1-score (o Dice coefficient) del Modelo B suele mantenerse similar o incluso mejorar ligeramente.

La diferencia más significativa entre las métricas de las clases corresponde al label "vehicle.car", demostrando que esta estrategia mejora de forma significativa el rendimiento del modelo en esta clase. Sin embargo, por lo general, ambos modelos demuestran un rendimiento similar. El hecho de que el modelo con merging obtenga valores de mF1 iguales o superiores, incluso cuando su mIoU es menor, sugiere que la simplificación del espacio semántico no deteriora la capacidad del modelo de capturar correctamente las instancias relevantes, sino que más bien reduce la penalización asociada a clases minoritarias con poca representatividad. Asimismo, al reducir el número total de clases consideradas, el promedio global de las métricas mejora, ya que se evita que clases irrelevantes o mal representadas distorsionen el cálculo final. Esto refuerza la utilidad práctica del merging en contextos donde no se requiere una segmentación tan granular, priorizando la estabilidad y representatividad de las métricas globales del modelo.



[4.1.2 Sustituye al With the inclusion of...]

Se puede observar que con la inclusión de las técnicas de regularización descritas para imágenes normales y BEV, el overfitting de los modelos ha sido reducido de forma significativa permitiendo entrenamientos más largos. Sin embargo, a pesar de que estas técnicas de regularización permiten que el loss en el conjunto de validación continúe descendiendo y permite mejorar el mean intersection over union de los modelos entrenados con imágenes normales, los resultados de mIoU de las imágenes BEV son ligeramente inferiores a los obtenidos con el modelo overfiteado. Esto puede deberse a que los conjuntos de validación y de test tienen el mismo tamaño (un 10% del conjunto total cada uno) estando compuestos por muy pocas imágenes (al rededor de 1600 cada uno). Al partir ambos del mismo conjunto inicial y compartir la misma distribución, puede darse el caso de que el overfitting en validación afecte al conjunto de test al no presentar una variabilidad significativa respecto al conjunto de validación. Esto puede explicar el porqué aquellas clases mayoritarias presentan mejores resultados con los modelos overfiteados tanto en imágenes normales como en BEV (ver Figura fusionada de RAW-SEG-BEV y RAW-BEV-SEG para todas las clases).

WARNING!!!!!!!!!!!!!!!!!!!!!!!
NECESITO LOS LOGS DE raw2bevseg_mit-b0_v0.5 y reentrenar raw2bevseg_mit-b0_v0.4 tanto para los logs como para los resultados. No me terminan de cuadrar las gráficas con el razonamiento y necesito que los modelos estén entrenados con los mismos hiperparámetros y el checkpoint strategy.



[4.2 3D detections evaluation]

Como se ha comentado anteriormente, el objetivo final del pipeline de anotación es el de generar máscaras de ocupación, oclusión y área conducible. Para realizar esto se toman máscaras semánticas del entorno tanto en el dominio de la cámara como en vista BEV y se realiza un proceso intermedio de detección 3D de objetos a partir de imágenes monoculares en el dominio de la cámara con el fin de estimar las dimensiones de los obstáculos. Esta detección 3D se realiza con algoritmos de clustering de nubes de puntos y resulta de especial interés el medir cómo de precisas son estas detecciones.

De hecho, el dataset de NuScenes fue desarrollado con el fin de entrenar y evaluar este tipo de estrategias entre otros, y cuenta con diversas métricas y métodos de evaluación para este propósito. Entre otras métricas, las que más destacan son las siguientes:
	- Mean Average Precision (mAP):

True positive metrics:
	- Average Translation Error (ATE):
	- Average Scale Error (ASE):
	- Average Orientation Error (AOE):
	- Average Velocity Error (AVE):
	- Average Attribute Error (AAE):
Estas métricas basadas en true positive detections se calculan empleando un threshold de 2m de distancia desde el centro y están diseñadas para ser escalares positivos (>= 0). Además, estas métricas están definidas por cada una de las clases semánticas del dataset, por lo que los resultados finales se toman con las medias de las métricas: mATE, mASE, mAOE, mAVE, y mAAE.

Custom metrics:
	- NuScenes detection score (NDS): 

<Explicar las métricas en detalle>

<Explicar el caso de uso: vista desde la cámara, no importa la orientación pero sí que hay que medir cuánto se equivoca en estimar las dimensiones y la posición de los cuboides. A la hora de estimar las posiciones, más que la distancia entre centroides interesa que la distancia volumétrica sea lo menor posible. Además, IoU por sí solo no tiene demasiado sentido, ya que dos predicciones se considerarán igual de malas si tienen IoU = 0 a pesar de que una esté más cerca del ground truth que la otra. En cuanto al error en la estimación de las posiciones, interesa el error en cada uno de los ejes para medir cómo se comporta el sistema en los giros. Con todo ello, se ha decidido utilizar la métrica que combina el IoU y la distancia volumétrica para medir el error en la traslación y la diferencia absoluta de las dimensiones por cada eje después de alinear la rotación de los cuboides para medir el error de las dimensiones>

Sin embargo, se ha decidido emplear unas métricas adaptadas para el caso de uso que se presenta en este trabajo...



El proceso de evaluación comienza definiendo... 


