¿Se ha explicado qué es una máscara semántica?

[4.1.3 Validation and comparison]

A la hora de validar y comparar distintos modelos de segmentación es fundamental utilizar una base de datos común y emplear métricas adecuadas que permitan evaluar el rendimiento de forma cuantitativa y objetiva. El primer requisito se garantiza mediante el uso del conjunto de test del dataset, el cual no ha sido previamente expuesto a los modelos a comparar, asegurando así una evaluación imparcial y representativa de su capacidad de generalización.

En cuanto a las métricas de evaluación, si observamos la Figura X, donde se muestra una máscara semántica inferida por un modelo de segmentación junto con la correspondiente máscara anotada (ground truth), podemos intuir que una predicción acertada será aquella que maximice la superposición entre la máscara predicha y la máscara real, evitando al mismo tiempo una sobreextensión excesiva más allá de los límites de las regiones de interés del ground truth.

Con este objetivo se emplean dos métricas distintas pero estrechamente relacionadas: los índices de Dice (también conocido como F1-score) y de Jaccard (también conocido como Intersection over Union, IoU), cuyas expresiones formales se presentan en la Fórmula X. Ambas métricas están acotadas en el rango [0, 1], donde un valor de 0 indica ausencia total de solapamiento entre predicción y ground truth, y un valor de 1 indica coincidencia perfecta entre ambas máscaras.

Supongamos que partimos de una imagen de resolución H×W y queremos predecir N clases distintas. La salida del modelo será un tensor de dimensiones HxWxN, es decir, N máscaras de tamaño HxW en las que cada elemento representa un valor de pertenencia a una clase determinada. Sin embargo, en la segmentación semántica se considera únicamente la mejor predicción para cada píxel, generando así N máscaras binarias que representan las clasificaciones semánticas finales de la imagen original. En este contexto, las métricas descritas se calculan de manera independiente para cada una de las N clases (hard metrics).

Para un ground truth fijo, ambas métricas están positivamente correlacionadas: si un clasificador A supera a un clasificador B según una de las métricas, también lo superará según la otra. Este hecho podría llevar a pensar que la elección entre una u otra métrica es arbitraria; sin embargo, las diferencias se hacen evidentes al calcular la media de las métricas sobre un conjunto de inferencias. En este punto, aunque ambas métricas coincidan en que el modelo A tiene mejor rendimiento que el modelo B, difieren en la magnitud de dicha diferencia.

En términos generales, la métrica IoU tiende a penalizar con mayor severidad las instancias mal clasificadas en comparación con el F1-score (o Dice score), proporcionando una medida más cercana al rendimiento en el peor de los casos. Por su parte, el F1-score tiende a reflejar un rendimiento más cercano al promedio. Esta diferencia implica que, al calcular promedios sobre múltiples inferencias, el IoU será más sensible a errores puntuales significativos, mientras que el F1-score será más tolerante ante estos casos aislados.

Con todo esto se ha elegido mean intersection over union como la métrica principal para evaluar y comparar modelos en este trabajo.



[5.1 Después de las medidas propuestas para solucionar el overfitting]

Además, es importante destacar la diferencia de métricas entre los dos modelos. A pesar de que el backbone y los hiperparámetros de entrenamiento se hayan mantenido iguales, es notoria la diferencia en los losses y mean intersection over union que presentan ambos modelos. Esto se debe principalmente a que los datasets de evaluación son distintos ya que el approach de segmentar primero una imagen para luego reproyectarla se evalúa con máscaras semánticas en el dominio de la cámara, mientras que el approach de reproyectar imágenees y luego segmentar se evalua directamente con máscaras en BEV.

Este hecho motiva la gráfica de la Figura 16, en la que se ha tomado el modelo 'raw2segbev_mit-b0_v0.3' y se ha evaluado con el conjunto de test primero con máscaras semánticas en el dominio de la cámara y luego con máscaras BEV. En esta Figura se puede apreciar en mayor detalle el valor de mean intersection over union por cada una de las clases semanticas para las que se ha entrenado el modelo y, en general, destacan dos hechos.

En primer lugar, existe un desbalanceo notable entre las métricas de las clases mayoritarias como 'backgroun' o 'flat.driveable_area' y el resto de clases minoritarias y, además, muchas otras clases tienen valores muy cercanos a 0 o nulos. Esto refleja en parte el desbalanceo inherente al dataset, siendo que hay clases semánticas que no tienen ningún pixel anotado en el mismo (ver Anexo A).

En segundo lugar, todos los valores de métricas son notablemente superiores en el modelo siendo evaluado con máscaras normales. Al realizar la reproyección de las máscaras a BEV ocurren tres hechos: se añade una zona de 'background'fija siendo el área BEV que está fuera del FOV de la cámara; se acorta la distancia máxima de la escena representada en la imagen BEV haciendo que únicamente los objetos presentes a una distancia menor a $15m$ (ver parámetros de reproyección en la Sección 3) estén presentes en la imagen, lo que acentúa que las clases minoritarias no se encuentren (aparece sobre todo el área conducible y los coches pero los peatones a no ser que se encuentren muy cerca y delante del vehículo no aparecen); y la resolución de la imagen disminuye a $1024x1024$. 

Todos estos factores hacen que la cantidad total de píxeles sea menor en el conjunto BEV provocando que los errores al cometer una mala predicción en un pixel sea notablemente más costoso que al evaluar con el dataset de máscaras normales (haciendo a su vez que las métricas medias sean mucho menores). 

Para solucionar esto, muchos métodos de entrenamiento emplean pesos asociados a cada clase para que a la hora de computar el loss en cada iteración del entrenamiento el efecto de cometer un error en la predicción de una clase minoritaria sea sustancialmente superior al predecir mal una clase mayoritaria. Sin embargo, debido a la naturaleza del proyecto, no es necesario tener una segmentación en clases tan precisas como las que proporciona el dataset, por lo que se ha decidido explorar si al realizar un merge de las clases semánticas se minimiza el efecto del desbalanceo de las clases.


[5.1.1 Merging labels]
Como se ha comentado, con el fin de mejorar el rendimiento de los modelos en las clases minoritarias del dataset se ha decidido hacer un merge de las clases semánticas en clases más representativas. Esta estrategia es la mima en la empleada en el pipeline de anotación implementado, por lo que se ha decidido implementarlo en tiempo de entrenamiento utilizando el mismo conjunto look up table de reglas de merging semánticos de labels que el descrito el la tabla 3 adaptando los scripts de entrenamiento y evaluación para considerar el merging look up table.

En la Figura X se presentan los resultados comparativos de dos modelos evaluados sobre el mismo conjunto de test con máscaras en vista BEV. El Modelo A ha sido entrenado con todas las clases semánticas originales, mientras que el Modelo B utiliza la estrategia de merging, reduciendo así el número total de clases consideradas. Los resultados muestran que, en algunas clases específicas, el Modelo A logra un mayor valor de Intersection over Union (IoU); sin embargo, en esos mismos casos, el F1-score (o Dice coefficient) del Modelo B suele mantenerse similar o incluso mejorar ligeramente.

La diferencia más significativa entre las métricas de las clases corresponde al label "vehicle.car", demostrando que esta estrategia mejora de forma significativa el rendimiento del modelo en esta clase. Sin embargo, por lo general, ambos modelos demuestran un rendimiento similar. El hecho de que el modelo con merging obtenga valores de mF1 iguales o superiores, incluso cuando su mIoU es menor, sugiere que la simplificación del espacio semántico no deteriora la capacidad del modelo de capturar correctamente las instancias relevantes, sino que más bien reduce la penalización asociada a clases minoritarias con poca representatividad. Asimismo, al reducir el número total de clases consideradas, el promedio global de las métricas mejora, ya que se evita que clases irrelevantes o mal representadas distorsionen el cálculo final. Esto refuerza la utilidad práctica del merging en contextos donde no se requiere una segmentación tan granular, priorizando la estabilidad y representatividad de las métricas globales del modelo.



[5.1.2 Sustituye al With the inclusion of...]

Se puede observar que con la inclusión de las técnicas de regularización descritas para imágenes normales y BEV, el overfitting de los modelos ha sido reducido de forma significativa permitiendo entrenamientos más largos. Sin embargo, a pesar de que estas técnicas de regularización permiten que el loss en el conjunto de validación continúe descendiendo y permite mejorar el mean intersection over union de los modelos entrenados con imágenes normales, los resultados de mIoU de las imágenes BEV son ligeramente inferiores a los obtenidos con el modelo overfiteado. Esto puede deberse a que los conjuntos de validación y de test tienen el mismo tamaño (un 10% del conjunto total cada uno) estando compuestos por muy pocas imágenes (al rededor de 1600 cada uno). Al partir ambos del mismo conjunto inicial y compartir la misma distribución, puede darse el caso de que el overfitting en validación afecte al conjunto de test al no presentar una variabilidad significativa respecto al conjunto de validación. Esto puede explicar el porqué aquellas clases mayoritarias presentan mejores resultados con los modelos overfiteados tanto en imágenes normales como en BEV (ver Figura fusionada de RAW-SEG-BEV y RAW-BEV-SEG para todas las clases).

WARNING!!!!!!!!!!!!!!!!!!!!!!!
NECESITO LOS LOGS DE raw2bevseg_mit-b0_v0.5 y reentrenar raw2bevseg_mit-b0_v0.4 tanto para los logs como para los resultados. No me terminan de cuadrar las gráficas con el razonamiento y necesito que los modelos estén entrenados con los mismos hiperparámetros y el checkpoint strategy.



[4.2.5.1 3D detections evaluation]

Como se ha comentado anteriormente, el objetivo final del pipeline de anotación es el de generar máscaras de ocupación, oclusión y área conducible. Para realizar esto se toman máscaras semánticas del entorno tanto en el dominio de la cámara como en vista BEV y se realiza un proceso intermedio de detección 3D de objetos a partir de imágenes monoculares en el dominio de la cámara con el fin de estimar las dimensiones de los obstáculos. Esta detección 3D se realiza con algoritmos de clustering de nubes de puntos y resulta de especial interés el medir cómo de precisas son estas detecciones.

De hecho, el dataset de NuScenes fue desarrollado con el fin de entrenar y evaluar este tipo de estrategias entre otros, y cuenta con diversas métricas y métodos de evaluación para este propósito. Entre otras métricas, las que más destacan son las siguientes:
	- Mean Average Precision (mAP): mAP es una métrica entre 0 y 1 que se utiliza ampliamente para evaluar el rendimiento de modelos de predicción. Esta métrica se suele definir como el area de la curva de precisión-recall, es decir, es la media de la integración de los valores precision-recal para cualquier valor de confianza de todas las clases disponibles. (explicar esto mejor). Las curvas de precision-recall se suelen obtener calculando las matrices de confusión para cada uno de los valores de confianza. A su vez, es muy común que estas matrices de confusión se calculen en base a la métrica de IoU entre la predicción y el ground truth de forma que si el valor de IoU supera un cierto treshold de confianza, se considera un true positive; si este valor es inferior se considera un false positive y si se tiene una predicción que no se puede asociar a ningún ground truth tendríamos un false negative. Es importante recalcar el proceso de asociación que tiene lugar antes de calcular las métricas: cada predicción se tiene que asociar con un groundtruth para calcular las métricas y, en el caso de las métricas de nuScenes, esta asociación se realiza considerando una cierta distancia entre centros de predicciones. En el caso de nuScenes, el mAP se calcula definiendo un match o asociación considerando únicamente la distancia 2D entre centros en el plano del suelo en lugar de considerar similitudes basadas en IoU. Este match entre predicciones y groundtruth se realiza para un cierto treshold de distancia entre centroides y para cada uno de ellos se calcula el average precision (AP) integrando la curva de precision-recall y finalmente se calcula el average sobre los thresholds [0.5, 1, 2, 4] metros para cada una de las clases y finalmente se calcula la media entre las clases.

Además del mAP, nuScenes también cuenta con métricas que miden los errores de traslación, escala, orientación, velocidad y el atributo predecido para el objeto (el atributo se define como una propiedad de una instancia que puede cambiar mientras que la categoría de esa instancia se mantiene igual, por ejemplo un coche puede estar aparcado, en movimiento o parado pero sigue siendo un coche). Estas métricas basadas en true positive detections se calculan empleando un threshold de 2m de distancia desde el centro y están diseñadas para ser escalares positivos (>= 0). Además, estas métricas están definidas por cada una de las clases semánticas del dataset, por lo que los resultados finales se toman con las medias de las métricas: mATE, mASE, mAOE, mAVE, y mAAE.
True positive metrics:
	- Average Translation Error (ATE): mide el error como la distancia euclidea entre los centroides en el plano del suelo.
	- Average Scale Error (ASE): se calcula como 1 - IoU después de alinear los centroides y las orientaciones.
	- Average Orientation Error (AOE): es la menor de las diferencias del yaw angle entre las instancias asociadas entre las predicciones y el ground truth.
	- Average Velocity Error (AVE): error absoluto de la velocidad en m/s.
	- Average Attribute Error (AAE): se calcula como 1 - acc donde acc es el accuracy en la clasificación del atributo.

Custom metrics:
	- NuScenes detection score (NDS): Además de todas estas métricas, nuScenes propone una métrica propia que se consolida como una suma ponderada de todas las anteriores.


Sin embargo, el caso de uso expuesto en este trabajo es ligeramente diferente. Al realizar detecciones 3D de objetos empleando imágenes monoculares y algoritmos de clústering, los cuboides 3D resultantes recogen unicamente los puntos visibles desde la cámara. Además, todos los cuboides 3D están alineados con la cámara por lo que calcular el error de orientación tampoco es de especial interés, ya sabemos que va a ser alto. Sin embargo, sí que interesa calcular el error al estimar las dimensiones y posiciones de los cuboides para saber si, con los datos disponibles, el pipeline implementado hace unas buenas detecciones.

De esta manera, en este caso de uso la distancia entre centroides de predicciones y ground truth no tiene mucho sentido ya que los cuboides del ground truth engloban a todo el objeto 3D, mientras que los cuboides de las predicciones solo recogen los puntos 3D visibles desde la cámara en cada frame. Por ello, en lugar de emplear la distancia entre centroides se ha decidido utilizar la distancia entre volúmenes o "volume to volume distance" (v2v). The metric v2v is defined as the shortest distance between the hull of one volume to the hull of another volume. Sin embargo, esta distancia es 0 cuando existe intersección entre ambos volúmenes, por lo que hay que considerar este caso y calcular otra métrica cuando se produzcan intersecciones. En este contexto nos encontramos con la métrica "Bounding Box Disparity" (BBD) [referencia], que combina IoU y v2v en una única métrica positiva contínua. Gracias a esta métrica se puede formalizar la dis-similaridad entre dos bounding boxes exista o no intersección: Since the Intersection over Union can only rank the similarity of two bounding boxes when they are overlapping, a distinction between two bounding boxes closer or farther away without an overlap cannot be made. De esta manera, si BBD = 0 no existe disparidad entre los bounding boxes (intersección total); si 0 < BBD < 1, hay disparidad pero tienen intersección; y si BBD >= 1, hay disparidad y no tienen intersección. Los casos en los que hay overlap o no se pueden visualizar en la Figura X.

Por otro lado, para calcular el error en la estimación de las dimensiones de los objetos 3D se ha decidido realizar la diferencia absoluta en dimensiones por cada eje. De esta manera se podrá evaluar los errores en cada uno de los ejes y visualizar en cuál de ellos se comete más error y en qué situaciones.



El proceso de evaluación comienza definiendo el área en el que se van a considerar las detecciones 3D resultado del pipeline. El pipeline devuelve detecciones en una distancia superior a la que luego es realmente considerada a la hora de generar las máscaras BEV de ocupación oclusión, por lo que se ha decidido acotar el rango de evaluación de la misma manera. Así, se calcula el polígono de visión de la cámara en el eje x de la cámara y se limita la distancia máxima a la que se van a considerar las detecciones a 15m. De esta manera, únicamente se van a tener en cuenta para la evaluación aquellas detecciones y ground truth que entren dentro de este polígono en un momento determinado. Con este primer filtrado se calculan las asociaciones entre predicciones y ground truth utilizando el algoritmo húngaro para resolver el problema Linear Sum Assignment Problem (LSAP). Con ello se obtienen las mejores asociaciones posibles entre instancias predecidas y ground truth, para las cuales se calculan cada una de las siguientes métricas:

tp, fp, fn, dds, deds, v2vs, vious, bbds

	- Confusion Matrix: se calculan los elementos de la matriz de confusión (true positives, como las detecciones que han sido asociadas; false positives, como las detecciones que no han sido asociadas; y false negatives, como aquellos elementos del ground truth que no han sido asociados).
	- Difference in dimensions (DD): es la diferencia entre volúmenes.
	- Difference in each dimension (DED): es la diferencia absoluta en cada uno de los ejes de los bounding boxes.
	- Volume to volume distance (v2v): distancia entre volúmenes.
	- Intersection Over Union (IoU): volumentric IoU.
	- Bounding Box Disparity (BBD): explicada anteriormente.


================================================================================================================
Como se ha comentado anteriormente, el objetivo final del pipeline de anotación es generar máscaras de ocupación, oclusión y área conducible. Para lograrlo, se parte de máscaras semánticas del entorno tanto en el dominio de la cámara como en vista cenital (BEV, Bird’s Eye View). Un paso intermedio crucial en este proceso es la detección 3D de objetos a partir de imágenes monoculares en el dominio de la cámara, con el fin de estimar las dimensiones y posiciones de los obstáculos. Esta detección se lleva a cabo mediante algoritmos de clustering de nubes de puntos y, dado su papel central, es especialmente relevante evaluar su precisión mediante métricas adecuadas.

El dataset nuScenes fue desarrollado, entre otros fines, para facilitar el entrenamiento y la evaluación de este tipo de estrategias de percepción 3D. Este conjunto de datos incluye diversas métricas estandarizadas para medir la calidad de las detecciones. Entre ellas, destaca en primer lugar la Mean Average Precision (mAP), una métrica ampliamente utilizada para evaluar modelos de detección. El mAP se define como el área bajo la curva de precision-recall, y refleja el equilibrio entre la precisión (proporción de verdaderos positivos respecto a todas las predicciones positivas) y el recall (proporción de verdaderos positivos respecto a todos los elementos relevantes del conjunto de datos).

Para calcular el mAP, es necesario establecer previamente un criterio de correspondencia entre las predicciones y el ground truth. En muchos casos este proceso se realiza mediante la métrica de Intersection over Union (IoU), que evalúa la superposición espacial entre predicción y realidad. Si el IoU supera un determinado umbral, se considera un true positive; si no, un false positive. Las instancias del ground truth sin predicciones asociadas se consideran false negatives. En el caso de NuScenes, sin embargo, la correspondencia no se basa en el IoU, sino en la distancia 2D entre los centros de las cajas en el plano del suelo. Así, para diferentes umbrales de distancia (0.5, 1, 2 y 4 metros), se calcula la Average Precision (AP) correspondiente, y el mAP final se obtiene como la media de las APs para todas las clases y umbrales considerados.

Además del mAP, nuScenes propone un conjunto de métricas adicionales que permiten evaluar de forma más específica distintos aspectos de las detecciones correctamente clasificadas (true positives). Estas métricas cuantifican errores en distintos atributos clave del objeto detectado, utilizando un umbral fijo de 2 metros para establecer la asociación entre predicción y ground truth. Las principales métricas de este tipo son:

    Average Translation Error (ATE): mide el error de traslación como la distancia euclídea entre los centros de las cajas predichas y las reales, proyectados en el plano del suelo.

    Average Scale Error (ASE): calcula el error de escala como 1−IoU, una vez alineadas la orientación y la posición de las cajas.

    Average Orientation Error (AOE): mide el error de orientación como la menor diferencia angular en el eje de guiñada (yaw) entre predicción y realidad.

    Average Velocity Error (AVE): representa el error absoluto en la estimación de la velocidad del objeto, medido en metros por segundo.

    Average Attribute Error (AAE): evalúa el error en la predicción de atributos dinámicos (por ejemplo, si un coche está aparcado, en movimiento o detenido) como 1−accuracy, siendo la accuracy la proporción de aciertos en la clasificación del atributo.

Como métrica global, NuScenes propone el NuScenes Detection Score (NDS), que sintetiza la calidad general de las detecciones mediante una combinación ponderada de todas las métricas anteriores. Esta métrica permite obtener una visión de conjunto del rendimiento del sistema, equilibrando precisión, localización, orientación, velocidad, escala y atributos.

El caso de uso abordado en este trabajo presenta particularidades que lo diferencian del escenario estándar planteado en el dataset NuScenes. En concreto, las detecciones 3D se obtienen a partir de imágenes monoculares mediante algoritmos de clustering, por lo que los cuboides 3D generados únicamente representan la parte visible del objeto desde la cámara en un instante determinado. Además, estos cuboides están siempre alineados con la orientación de la cámara, lo que hace que el cálculo del error de orientación carezca de utilidad práctica en este contexto, ya que se sabe de antemano que este error será elevado. No obstante, resulta de gran interés evaluar la precisión en la estimación de las posiciones y dimensiones de los cuboides generados, ya que ello permite valorar si el pipeline es capaz de realizar detecciones razonablemente precisas con la información disponible.

En este escenario, emplear la distancia entre centroides como criterio de asociación entre predicciones y ground truth resulta problemático. Mientras que los cuboides del ground truth engloban la totalidad del objeto en 3D, los cuboides predichos solo contienen los puntos visibles desde la cámara, lo que genera desplazamientos sistemáticos en las posiciones relativas de los centros. Por ello, se ha optado por utilizar una métrica más adecuada: la distancia entre volúmenes (volume-to-volume distance, o v2v), definida como la distancia mínima entre las envolventes convexas de los volúmenes considerados.

Sin embargo, la métrica v2v presenta una limitación importante: toma un valor de cero siempre que exista intersección entre los volúmenes, independientemente del grado de solapamiento. Para superar esta limitación se recurre a la métrica Bounding Box Disparity (BBD) [referencia], que combina IoU y v2v en una única métrica continua y no negativa. Esta métrica permite cuantificar la disimilitud entre dos cuboides, tanto si se solapan como si no. Como se indica en la definición original, mientras que el IoU únicamente puede comparar cuboides con intersección, el BBD proporciona una medida de disparidad también en ausencia de solapamiento. Su interpretación es la siguiente:

    Si BBD = 0, los cuboides son idénticos (intersección total).

    Si 0 < BBD < 1, existe solapamiento pero también disparidad.

    Si BBD ≥ 1, los cuboides no se solapan.

Estos distintos casos se ilustran en la Figura X.

Por otra parte, para evaluar el error en la estimación de las dimensiones de los objetos detectados, se ha decidido calcular la diferencia absoluta entre las dimensiones de los cuboides predichos y del ground truth en cada uno de los ejes (longitud, anchura y altura). Esto permite analizar en qué dimensión se concentra mayor error y bajo qué condiciones.

El proceso de evaluación comienza delimitando el área en la que se considerarán las detecciones generadas por el pipeline. Dado que el sistema produce predicciones a distancias mayores que las empleadas posteriormente para generar las máscaras BEV de ocupación y oclusión, se ha decidido restringir el rango de evaluación a la misma región que se usa para la generación de dichas máscaras. Concretamente, se define el polígono de visión de la cámara proyectado sobre el plano horizontal y se limita la distancia máxima de detección a 15 metros. De este modo, solo se consideran para la evaluación aquellas detecciones y elementos del ground truth que se encuentren dentro de este polígono en el instante correspondiente.

Una vez filtradas las detecciones, se procede a la asociación entre predicciones y ground truth mediante la resolución del Linear Sum Assignment Problem (LSAP) utilizando el algoritmo húngaro. Esto permite encontrar las correspondencias óptimas entre objetos predichos y reales, sobre las cuales se calculan las siguientes métricas:

    True Positives (TP), False Positives (FP) y False Negatives (FN): determinados en función de las asociaciones establecidas.

    DD (Difference in Dimensions): diferencia de volumen entre cuboides asociados.

    DED (Difference in Each Dimension): diferencia absoluta por eje (longitud, anchura, altura).

    v2v: distancia entre volúmenes.

    IoU: intersección sobre unión en 3D.

    BBD (Bounding Box Disparity): métrica continua de disimilitud explicada anteriormente.

Estas métricas permiten evaluar de manera más precisa y adaptada la calidad de las detecciones 3D en el contexto particular del pipeline propuesto, considerando las limitaciones y características propias del proceso de detección monocular.
================================================================================================================



[4.2.5.2 BEV masks evaluation]

Para evaluar las máscaras de ocupación, oclusión y área conducible es necesario generar un ground truth con el que comparar la salida del pipeline de anotación. Para ello



[5.1.2 Data Augmentations]

El objetivo de esta sección es dual. Por el lado de la estrategia de entrenar los modelos de segmentacion semántica con imágenes en el dominio de la cámara, se quiere comprobar el efecto de aplicar las transformaciones geométricas como método de data augmentation technique para ver si se reduce el overfitting. Y, por otro lado, se quiere explorar cual de las dos técnicas descritas es mejor para realizar data augmentations de imágenes BEV: si el aplicar las mismas aumentaciones de datos que las geométricas para imágenes normales o realizar una modificación de los parámetros extrínsecos de la cámara antes de realizar las proyecciones.

En este contexto, con la introducción de random cropping, horizontal flipping and rescaling augmentation strategies en imágenes normales, el comportamiento de overfitting se ve significativamete reducido como se muestra en la Figura X. Se puede observar además, que esta reducción del overfitting permite realizar entrenamientos más largos y mejorar el rendimiento del modelo en el conjunto de evaluación haciendo que converga en valores más elevados de mIoU. Este hecho, además, también se refleja en la evaluación de los modelos sobre el conjunto de test, en el que el modelo con aumento de datos supera al modelo que sufre de overfitting.

En segundo lugar, al analizar la gráfica del proceso de entrenamiento de tres modelos entrenados con imágenes BEV, se observan diferencias significativas en el comportamiento del entrenamiento. El primer modelo, sin aumento de datos, exhibe un overfitting notable. En contraste, las estrategias de aumento de datos, tanto las tradicionales (volteo aleatorio, recorte aleatorio y reescalado) como la estrategia personalizada de modificar los parámetros extrínsecos de la cámara, logran reducir este overfitting.

El modelo que emplea las técnicas de aumento de datos tradicionales muestra un error generalmente más alto durante el entrenamiento, con curvas de error de entrenamiento y validación muy cercanas y un mIoU en el conjunto de validación considerablemente inferior al de los otros dos modelos, lo que sugiere un posible underfitting y parece alinearse con la hipótesis inicial de que al aplicar estas estrategias el modelo puede no contar con suficiente información para el entrenamiento.

Por otro lado, el modelo entrenado con la estrategia de aumento de datos personalizada, basada en la modificación de los parámetros extrínsecos de la cámara, muestra un overfitting mínimo. Esto permite un entrenamiento más prolongado y mejores resultados en el conjunto de validación, superando en rendimiento al modelo con overfitting. En general, los resultados sugieren que las técnicas de aumento de datos personalizadas para imágenes BEV ofrecen un rendimiento superior a las técnicas tradicionales aplicadas directamente a estas imágenes. Sin embargo, es necesario atender a la evaluación en el conjunto de test de los modelos. En la Figura X se muestra esta evaluación, y se puede observar como la diferencia entre los tres modelos no es tan significativa realmente. A pesar de que el modelo entrenado con modificación de extrínsecos presenta unos resultados algo superiores, la diferencia del rendimiento entre los otros dos modelos no es tan clara siendo en ciertos casos el modelo con aumentación de datos normales superior al modelo overfiteado en las clases relativas a vehículos.



[5.1.3 Raw2SegBEV vs Raw2BEVSeg]






[5.2 Annotation evaluation]

Para realizar la evaluación del pipeline se ha tomado una escena anotada de NuScenes. Concretamente es la escena 'scene-0061', que tiene anotaciones de objetos, el mapa y odometría cada 2Hz sumando un total de 39 frames anotados. Concretamente, esta escena está capturada en 'singapore onenorth' y <introducir descripción de la escena> (Parked truck, construction, intersection, turn left, following a van). El tramo que realiza el ego.vehicle es el que se muestra en la Figura X.

El vehículo comienza al lado de un camión que es visible por la cámara frontal durante los 4 primeros frames (siendo que en el cuarto solo se visualiza una pequeña esquina del camión). Además, en el segundo frame de la escena ya se visualiza un vehículo en el carril derecho y a partir del tercer frame se comienza a visualizar la furgoneta que toma la salida de la izquierda y que el ego vehicle seguirá hasta el final de la escena. Durante todo el tramo se producen varias oclusiones y visualizaciones parciales de los objetos, por lo que es una situación lo suficientemente representativa como para evaluar el output del pipeline y comprobar los puntos fuertes y carencias del mismo.

Por otro lado, cada objeto de la escena, tanto anotaciones del ground truth como las detecciones realizadas, cuentan con un identificador único y, además, como se especificó en la Sección X, el proceso de evaluación realiza una asociación entre las detecciones y los objetos anotados, resultando en que por cada frame existe un número variable de métricas obtenidas (cada métrica se obtiene entre par anotación ground-truth y detección pipeline). Esta evaluación se ha realizado con un FOV range de 30 metros, se ha empleado la distancia entre volúmenes v2v como métrica de asociación y se ha establecido la distancia máxima de asociación en 3.0 metros. Con estos parámetros, se ha obtenido la matriz de confusión mostrada en la Tabla X que recoge todos los casos entre las detecciones y el ground truth visibles durante todos los frames de la escena. En esta tabla, un True positive representa aquellas detecciones realizadas que han sido asociadas a un ground truth; un False Positive representa aquellas detecciones que no han sido asociadas y un False Negative representa aquellos objetos del ground truth que no han sido detectados. Con esta matriz de confusión acumulada a lo largo de todos los frames, se puede observar que del total de 107 objetos de ground truth que están en el rango de visión de la cámara, el sistema consigue realizar estimaciones de 92 de ellos, dejando 15 objetos de ground truth o bien con predicciones muy ruidosas o bien sin detectar. Sin embargo, también es destacable que en varias ocasiones el sistema ha realizado predicciones muy ruidosas (en la mayoría de los casos debido a que no se ha filtrado correctamente las nubes de puntos de los objetos), detectando 23 objetos que realmente no existen en el ground truth. En general, The Precision of the pipeline is approximately 80.0%. This indicates that when the system predicts an object, it is correct 80% of the time, suggesting a relatively low rate of false positives. y The Recall is approximately 86.0%. This means the pipeline successfully detected 86% of all actual 3D objects present in the scene, demonstrating its ability to find most of the relevant objects. 

Entre todos los True positives, se ha realizado una evaluación en la estimación de dimensiones recogida en la Tabla X. Además, en esta tabla también se muestran las métricas de los objetos y frames pero agrupadas por su alineación con la cámara frontal del vehículo. Esta agrupación se calcula considerando la diferencia en el yaw angle entre los objetos anotados del ground truth y la orientación del vehículo. Si esta diferencia en ángulos sobrepasa los 15 grados, se considera que el objeto no está alineado con la cámara frontal.

Si atendemos a la media total de la diferencia en dimensiones por cada uno de los ejes, comprobamos que el eje X tiene un índice de error notablemente superior a los ejes Y y Z del sistema de coordenadas del vehículo. Siendo que el eje X en el sistema de coordenadas del vehículo se corresponde con el eje de profundidad de la cámara, este hecho parece corresponderse a que la estimación del depht de los objetos es bastante inferior a la estimación del alto y ancho. Esto además se refuerza con las medias agrupadas por alineación con la cámara o no: los objetos alineados con la cámara destacan por tener un error en la estimación de profundidad notablemente superior mientras que las estimaciones de alto y ancho se ajustan bastante al ground truth (medias de 43cm y 27cm en los errores de estimación de anchura y altura respectivamente ); mientras que aquellos objetos que no se alinean con la cámara tienen índices de error superiores no solo en profundidad si no que en todos los ejes. Este hecho refleja la sobre-estimación del footprint en ancho y profundidad cuando los objetos no se alinean con la cámara como se muestra en el caso particular de la Figura X. Sin embargo, en este último caso, cabría esperar que el error en altura se mantuviera más o menos similar a pesar de que la alineación del objeto con la cámara no se diera, ya que la altura debería de mantenerse similar (el objeto solo está desalineado con rotación en el eje Z). Para ello, se ha realizado un análisis con más detalle atendiendo a las métricas de cada uno de los objetos del ground truth para los que se han realizado detecciones. 

Los casos más destacables que reflejan distintos aspectos del funcionamiento del sistema de anotación para la estimación de las dimensiones de objetos son los casos del camión aparcado al inicio de la escena (con id 41); la furgoneta que se mantiene delante del ego-vehicle (que tiene el id 154) y una excavadora que se encuentra parada a la mitad de la curva de la escena (id 129) (ver Figura X en la que se encuentran representados estos tres objetos). La evolucion de los errores en las estimaciones de dimensiones de los objetos por cada frame se pueden observar en la Figura X, donde además aparece representada en una barra horizontal si en el frame actual el objeto está alineado con la cámara o no.

En el caso del camión aparcado, es destacable como el error en la estimación de longitud es de aproximadamente 2 metros, pero en los últimos frames este error se dispara hasta casi los 7 metros. Esto se debe principalmente a que debido a que el camión está aparcado y el ego-vehicle lo sobrepasa, la zona visible del camión cambia considerablemente en cada frame, siendo el frame 2 en el que más información visible se extrae y siendo el frame 5 el frame en el que menos información se puede extraer ya que solo es visible el lateral de la parte frontal del camión, provocando que esa estimación instantánea de la produndidad del vehículo tenga mucho error.

Con la evolución del error en dimensiones de la furgoneta, se puede comprobar el comportamiento típico del sistema cuando un objeto está alineado con la cámara o no. En los primeros frames, la furgoneta se encuentra alineada con la cámara, resultando en un error alto en la estimación de la longitud del objeto, pero un error más bajo en las estimaciones de altura y anchura. Sin embargo, la furgoneta comienza a rotar progresivamente respecto a la cámara, haciendo que el error en la estimación de longitud de la misma se vea diluido con el aumento del error en la encimación del ancho de la furgoneta mientras que el error en la estimación de la altura se mantiene relativamente constante (a excepción de algunos outliers). Y cuando la furgoneta se vuelve a alinear con la cámara, el error predominante vuelve a ser el de la estimación de longitud del objeto. Esto demuestra una de las mayores limitaciones del sistema de anotación al emplear Axis Aligned Bounding Boxes.

Finalmente, el caso de la excavadora aparcada es destacable ya que presenta un error en la estimación de la altura inusualmente alto. Durante prácticamente todos los frames en los que este objeto es visible por el ego vehicle, el error su estimación de altura se mantiene por encima de los 2 metros. Sin embargo, este caso se relaciona estrechamente con el caso del camión: si atendemos a la Figura X en la que se visualiza el ground truth del objeto junto con su detección en el frame X, podemos observar que hay una clara oclusión de la excavadora debido a que la furgoneta se interpone haciendo que solo la parte superior de la excavadora sea visible y, por ende, afectando notablemente a la estimación de la altura de la misma.


Por otro lado, el Bounding Box Disparity es una métrica muy útil para identificar si las posiciones de los cuboides son adecuadas o no y permite identificar de forma sencilla posibles malas detecciones. Por ejemplo, podemos observar la evolución del BBD durantes los frames de la escena para el caso de la furgoneta en la Figura X. En esta gráfica, la linea horizontal representa el límite entre IoU y V2V propio de la métrica BBD. Por ejemplo, la primera detección de la furgoneta en el frame 2 presenta un BBD mayor que 1, lo que indica que la detección no tiene intersección con el bounding box del ground truth como se muestra en la Figura X. Esto en general puede indicar una detección errónea como son los casos de los frames 12 y 13.

Sin embargo, este hecho también puede ser un síntoma de una estimación de profundidad errónea como es el caso de la furgoneta en el frame 2 (ver Figura X) en el que la estimación de profundidad es inferior a la real, pero las dimensiones relativas de la furgoneta se mantienen.

De esta manera, el BBD se constituye como una métrica adecuada para identificar posibles outliers de detecciones en el caso que se supere BBD > 1 o, si se mantiene por debajo de 1 se puede considerar como un valor inverso de confianza de la predicción: cuanto menor sea el BBD mejor estimación del Bounding Box. Así, se puede observar que la mayor parte de las detecciones hechas para la furgoneta presentan un BBD inferior a 1, rondando un valor medio de X, con lo que se puede decir que a pesar de que los bounding boxes predecidos no se ajusten exactamente al ground truth su posicionamiento es relativamente aceptable a lo largo de toda la escena. 




[5.2.4 Annotation evaluation: semantic masks]

Con el objetivo de evaluar los resultados finales generados por el pipeline de anotación desarrollado, se ha llevado a cabo una comparación entre las máscaras de ocupación, oclusión y área conducible generadas por el sistema y aquellas obtenidas a partir del *ground truth* empleando un procedimiento de *ray casting* (ver Sección X). Este análisis busca determinar la precisión del pipeline mediante métricas objetivas, basadas en la similitud entre las salidas generadas automáticamente y las referencias anotadas manualmente.

Las máscaras proporcionadas por el pipeline se construyen a partir de diferentes fuentes de información. Las máscaras de área conducible y de oclusión se obtienen desde las máscaras semánticas proyectadas en vista cenital (BEV), donde la oclusión se representa mediante las máscaras semánticas distorsionadas de los objetos presentes en la escena (ver Sección X). Por otro lado, las máscaras de ocupación se generan como la base de los cuboides asociados a las detecciones 3D realizadas por el sistema. Esta diferencia en la fuente y el proceso de obtención es clave para entender el comportamiento observado en las métricas de evaluación.

En la Figura X se presenta el valor de mIoU (mean Intersection over Union) por clase semántica a lo largo de todos los *frames* de la escena. Se observa que la clase 'background' alcanza los mayores valores de IoU. Este comportamiento es esperable, dado que una gran parte del área clasificada como fondo corresponde a zonas no visibles por las cámaras, las cuales permanecen fijas tanto en las máscaras de *ground truth* como en las generadas por el sistema. En contraste, las máscaras de ocupación presentan en general los menores valores de IoU.

La baja precisión observada en las máscaras de ocupación puede estar relacionada con problemas en las detecciones 3D mencionados en análisis anteriores. Esta limitación también podría explicar por qué el IoU de oclusión supera al de ocupación, a pesar de que, en principio, ambas máscaras deberían estar correlacionadas. Teóricamente, una máscara de ocupación deficiente debería implicar una máscara de oclusión igualmente deficiente. No obstante, en este caso, como las máscaras de oclusión se derivan directamente de las máscaras semánticas en BEV, el error en las detecciones 3D no impacta de forma directa en su generación, lo que podría estar provocando esta discrepancia.

En cuanto al área conducible, el comportamiento del IoU a lo largo del tiempo refleja una relación directa entre la complejidad de la escena y el rendimiento del pipeline. Durante los primeros *frames* (0–10), se observa un IoU consistentemente alto, lo cual coincide con un entorno de conducción simple: una carretera abierta con pocos obstáculos. Sin embargo, a partir del *frame* 11, y especialmente entre los *frames* 11 y 23 —zona correspondiente a una curva—, se produce una degradación progresiva del IoU. Esta disminución no se debe únicamente al acto de girar, sino a que dicha curva coincide con una intersección compleja, donde la presencia de más vehículos, peatones y objetos cercanos genera oclusiones más numerosas y contornos del área conducible mucho más irregulares.

Esta mayor complejidad expone las limitaciones del pipeline en dos niveles. Por un lado, incrementa el ruido en la segmentación semántica, que tiene mayores dificultades para identificar de forma precisa las zonas transitables cuando están fragmentadas por obstáculos. Por otro lado, resalta la divergencia entre la predicción generada por el modelo —basada en la apariencia visual de la escena— y el *ground truth*, que define las áreas conducibles mediante polígonos idealizados y geométricamente perfectos. La combinación de estos factores —densidad escénica elevada y discrepancia entre representación visual y referencia estructurada— explica el descenso del rendimiento observado, y se alinea con los resultados obtenidos en la evaluación de la precisión 3D expuestos en la Sección X.










